{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A Friendly Guide to integrate Airflow with BigQuery Airflow is a popular workflow tool for scheduling and monitoring the ETL(Extract, Transform, Load) process in modern data engineering. It allows users to create a data pipeline between various data sources such as RDBMS, API, spreadsheet, and data warehouse. BigQuery , Google's managed data warehouse, is one of the most frequently used data warehouses for its flexibility and convenience. This guide introduces how to create a simple batch ETL pipeline by integrating Airflow with BigQuery. The following tutorial is appropriate for users who have a basic experience with the ETL process, Bigquery, and writing Airflow DAG, Requirements Docker Desktop Google Cloud Platform account and project Goals Create a simple DAG that writes a query result to a BigQuery table Create basic validation tasks using Airflow Operators Non-goals Deploy Airflow on a cloud environment Cover complicated data transformation process Integrate with external data sources other than BigQuery","title":"Introduction"},{"location":"#a-friendly-guide-to-integrate-airflow-with-bigquery","text":"Airflow is a popular workflow tool for scheduling and monitoring the ETL(Extract, Transform, Load) process in modern data engineering. It allows users to create a data pipeline between various data sources such as RDBMS, API, spreadsheet, and data warehouse. BigQuery , Google's managed data warehouse, is one of the most frequently used data warehouses for its flexibility and convenience. This guide introduces how to create a simple batch ETL pipeline by integrating Airflow with BigQuery. The following tutorial is appropriate for users who have a basic experience with the ETL process, Bigquery, and writing Airflow DAG,","title":"A Friendly Guide to integrate Airflow with BigQuery"},{"location":"#requirements","text":"Docker Desktop Google Cloud Platform account and project","title":"Requirements"},{"location":"#goals","text":"Create a simple DAG that writes a query result to a BigQuery table Create basic validation tasks using Airflow Operators","title":"Goals"},{"location":"#non-goals","text":"Deploy Airflow on a cloud environment Cover complicated data transformation process Integrate with external data sources other than BigQuery","title":"Non-goals"},{"location":"01-set-up/","text":"Set Up Deploy Airflow on Local Environment This guide uses docker-compose to deploy Airflow on the local environment to help you start from scratch. If you already have Airflow deployed on a cloud environment, please skip this part. Make sure you have Docker Desktop engine running on the local environment before installation. # download docker-compose.yaml for Airflow curl -LfO 'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml' mkdir -p ./dags ./logs ./plugins echo -e \"AIRFLOW_UID= $( id -u ) \" > .env # execute init process docker-compose up airflow-init # install all components docker-compose up Visit localhost:8080 in the browser and log in with default credentials. id: airflow password: airflow Now, you will see the list of example DAGs. Refer to the official document of apache airflow for more information. Create Raw BigQuery Table You will need a BigQuery table as a raw data source in your GCP project. We will use the bigquery-public-data.austin_bikeshare.bikeshare_trips dataset to create a dummy table. CREATE TABLE { your_dataset_name } . trips AS ( SELECT bikeid , start_time , duration_minutes FROM bigquery - public - data . austin_bikeshare . bikeshare_trips );","title":"Set Up Environment"},{"location":"01-set-up/#set-up","text":"","title":"Set Up"},{"location":"01-set-up/#deploy-airflow-on-local-environment","text":"This guide uses docker-compose to deploy Airflow on the local environment to help you start from scratch. If you already have Airflow deployed on a cloud environment, please skip this part. Make sure you have Docker Desktop engine running on the local environment before installation. # download docker-compose.yaml for Airflow curl -LfO 'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml' mkdir -p ./dags ./logs ./plugins echo -e \"AIRFLOW_UID= $( id -u ) \" > .env # execute init process docker-compose up airflow-init # install all components docker-compose up Visit localhost:8080 in the browser and log in with default credentials. id: airflow password: airflow Now, you will see the list of example DAGs. Refer to the official document of apache airflow for more information.","title":"Deploy Airflow on Local Environment"},{"location":"01-set-up/#create-raw-bigquery-table","text":"You will need a BigQuery table as a raw data source in your GCP project. We will use the bigquery-public-data.austin_bikeshare.bikeshare_trips dataset to create a dummy table. CREATE TABLE { your_dataset_name } . trips AS ( SELECT bikeid , start_time , duration_minutes FROM bigquery - public - data . austin_bikeshare . bikeshare_trips );","title":"Create Raw BigQuery Table"},{"location":"02-authenticate/","text":"Authenticate Google Cloud Connection To integrate Airflow with BigQuery, you need to set up a connection between GCP and Airflow. Create a GCP Service Account First, you need a GCP service account with specific permissions such as bigquery.jobs.create , bigquery.tables.getData and bigquery.tables.create . Visit IAM & Admin > Service Accounts on GCP console and create a service account with the following role. BigQuery User BigQuery Data Editor After saving the service account, navigate to Actions > Mnanage Keys . Go to ADD KEY > Create new key and select the key type as JSON . It will automatically download json file on local device. Add Airflow Connection On Airflow UI, go to Admin > Connections . Create a connection with following configuration. Connection Id : Your preferable connection id (ex. google-cloud-conn-id) Connection Type : Google Cloud Project Id : Your GCP project id Keyfile JSON : Full content of your service account key json file Now you're ready to write DAG!","title":"Authenticate Google Cloud Credentials"},{"location":"02-authenticate/#authenticate-google-cloud-connection","text":"To integrate Airflow with BigQuery, you need to set up a connection between GCP and Airflow.","title":"Authenticate Google Cloud Connection"},{"location":"02-authenticate/#create-a-gcp-service-account","text":"First, you need a GCP service account with specific permissions such as bigquery.jobs.create , bigquery.tables.getData and bigquery.tables.create . Visit IAM & Admin > Service Accounts on GCP console and create a service account with the following role. BigQuery User BigQuery Data Editor After saving the service account, navigate to Actions > Mnanage Keys . Go to ADD KEY > Create new key and select the key type as JSON . It will automatically download json file on local device.","title":"Create a GCP Service Account"},{"location":"02-authenticate/#add-airflow-connection","text":"On Airflow UI, go to Admin > Connections . Create a connection with following configuration. Connection Id : Your preferable connection id (ex. google-cloud-conn-id) Connection Type : Google Cloud Project Id : Your GCP project id Keyfile JSON : Full content of your service account key json file Now you're ready to write DAG!","title":"Add Airflow Connection"},{"location":"03-write-dag/","text":"Write DAG with ETL Tasks In this part, we will write a simple DAG that performs an ETL task with BigQueryOperator . BigQueryOperator executes BigQuery SQL query and saves the result to the destination table, a great way to create a pipeline that loads a data mart from another table. You will need two files under the ./dags directory: tutorial_dags.py : we will define the DAG that contains BigQueryOperator. query.sql : we will write a query to load data. Refer to the GitHub Repository for full code. Create a Model Object with BigQueryOperator First, create a DAG model with BigQueryOperator in tutorial_dags.py . from airflow import models from airflow.contrib.operators.bigquery_operator import BigQueryOperator project_id = ' {your_project_id} ' destination_table_id = ' {your_dataset_id} .long_trips' with models . DAG ( dag_id = \"test_etl_dag\" , schedule = \"@once\" , start_date = datetime ( 2022 , 12 , 1 ), catchup = False , tags = [ \"example\" , \"bigquery\" ], ) as dag : run_etl = BigQueryOperator ( task_id = 'run_etl' , sql = 'query.sql' , destination_dataset_table = destination_table_id , write_disposition = 'WRITE_TRUNCATE' , gcp_conn_id = ' {your_custom_conn_id} ' , use_legacy_sql = False ) Some of the basic parameters are like following: sql : The SQL query to be executed. It can be either a file path or a raw string of SQL query. destination_dataset_table : BigQuery table that will store the results of the query. (ex. <dataset>.<table> ) write_dispoistion : Specifies the action that occurs if the destination table already exists. WRITE_EMPTY : Write data if the destination table is empty. WRITE_TRUNCATE : Overwrite the destination table data. WRITE_APPEND : Append to the existing table data. gcp_conn_id : Reference to google cloud connection. For more information on BigQueryOperator , please refer to the official documentation . Write Query In query.sql , we will define a simple query that filters row with duration_minutes of more than 100. SELECT * FROM test_dataset . trips WHERE duration_minutes > 100 ; We will not cover complicated SQL syntax or transformation processes in this guide, but you can always look up more variations. Now that we have our DAG, let's run and test it!","title":"Write DAG with ETL Tasks"},{"location":"03-write-dag/#write-dag-with-etl-tasks","text":"In this part, we will write a simple DAG that performs an ETL task with BigQueryOperator . BigQueryOperator executes BigQuery SQL query and saves the result to the destination table, a great way to create a pipeline that loads a data mart from another table. You will need two files under the ./dags directory: tutorial_dags.py : we will define the DAG that contains BigQueryOperator. query.sql : we will write a query to load data. Refer to the GitHub Repository for full code.","title":"Write DAG with ETL Tasks"},{"location":"03-write-dag/#create-a-model-object-with-bigqueryoperator","text":"First, create a DAG model with BigQueryOperator in tutorial_dags.py . from airflow import models from airflow.contrib.operators.bigquery_operator import BigQueryOperator project_id = ' {your_project_id} ' destination_table_id = ' {your_dataset_id} .long_trips' with models . DAG ( dag_id = \"test_etl_dag\" , schedule = \"@once\" , start_date = datetime ( 2022 , 12 , 1 ), catchup = False , tags = [ \"example\" , \"bigquery\" ], ) as dag : run_etl = BigQueryOperator ( task_id = 'run_etl' , sql = 'query.sql' , destination_dataset_table = destination_table_id , write_disposition = 'WRITE_TRUNCATE' , gcp_conn_id = ' {your_custom_conn_id} ' , use_legacy_sql = False ) Some of the basic parameters are like following: sql : The SQL query to be executed. It can be either a file path or a raw string of SQL query. destination_dataset_table : BigQuery table that will store the results of the query. (ex. <dataset>.<table> ) write_dispoistion : Specifies the action that occurs if the destination table already exists. WRITE_EMPTY : Write data if the destination table is empty. WRITE_TRUNCATE : Overwrite the destination table data. WRITE_APPEND : Append to the existing table data. gcp_conn_id : Reference to google cloud connection. For more information on BigQueryOperator , please refer to the official documentation .","title":"Create a Model Object with BigQueryOperator"},{"location":"03-write-dag/#write-query","text":"In query.sql , we will define a simple query that filters row with duration_minutes of more than 100. SELECT * FROM test_dataset . trips WHERE duration_minutes > 100 ; We will not cover complicated SQL syntax or transformation processes in this guide, but you can always look up more variations. Now that we have our DAG, let's run and test it!","title":"Write Query"},{"location":"04-run-dag/","text":"Run pipeline Trigger DAG Place tutorial_dag.py and query.sql under the ./dags directory like below. \u251c\u2500\u2500 dags \u2502 \u251c\u2500\u2500 tutorial_dag.py \u2502 \u2514\u2500\u2500 query.sql After a slight delay, you can search the DAG by dag_id on Airflow UI. Activate and trigger the DAG manually. Check Data on BigQuery After the DAG finishes successfully, check the BigQuery destination table to see the data mart as a result.","title":"Run DAG on Airflow"},{"location":"04-run-dag/#run-pipeline","text":"","title":"Run pipeline"},{"location":"04-run-dag/#trigger-dag","text":"Place tutorial_dag.py and query.sql under the ./dags directory like below. \u251c\u2500\u2500 dags \u2502 \u251c\u2500\u2500 tutorial_dag.py \u2502 \u2514\u2500\u2500 query.sql After a slight delay, you can search the DAG by dag_id on Airflow UI. Activate and trigger the DAG manually.","title":"Trigger DAG"},{"location":"04-run-dag/#check-data-on-bigquery","text":"After the DAG finishes successfully, check the BigQuery destination table to see the data mart as a result.","title":"Check Data on BigQuery"},{"location":"05-validation/","text":"Validation There is a lot of ways to validate the ETL process and results. For example, dbt is a great way to validate the pipeline. However, you can utilize Airflow Operators such as BigQueryValueCheckOperator or BigQueryCheckOperator to add a simple validation task to your DAG. BigQueryValueCheckOperator from airflow.providers.google.cloud.operators.bigquery import BigQueryValueCheckOperator check_value = BigQueryValueCheckOperator ( task_id = \"check_value\" , sql = 'validation_query.sql' , pass_value = 0 , gcp_conn_id = 'google_cloud_conn_id' , use_legacy_sql = False , ) This operator checks if the result of the SQL query is equal to pass_value . For example, if you want to validate that none of the rows in target table has duration_minutes value less than 100, you could write following query and set pass_value to 0. -- validation_query.sql SELECT count ( * ) FROM test_dataset . long_trips WHERE duration_minutes < 100 BigQueryCheckOperator from airflow.providers.google.cloud.operators.bigquery import BigQueryCheckOperator check_value = BigQueryCheckOperator ( task_id = \"check_value\" , sql = 'validation_query.sql' , gcp_conn_id = 'google_cloud_conn_id' , use_legacy_sql = False , ) This operator checks if every row of the result of the SQL query is true . For example, if you want to validate that at least one row in the target table have duration_minutes bigger than 1000, you can write the following query. -- validation_query.sql SELECT count ( * ) FROM test_dataset . long_trips WHERE duration_minutes > 1000 This validation will only fail if the count(*) == Full Code Following DAG executes ETL process with 2 validation tasks. # tutorial_dag_with_validation.py from __future__ import annotations from datetime import datetime from airflow import models from airflow.contrib.operators.bigquery_operator import BigQueryOperator from airflow.providers.google.cloud.operators.bigquery import \\ BigQueryValueCheckOperator , BigQueryCheckOperator project_id = \"starlit-sum-372013\" destination_table_id = \"test_dataset.long_trips\" with models . DAG ( dag_id = \"test_etl_dag_with_validation\" , schedule = \"@once\" , start_date = datetime ( 2022 , 12 , 1 ), catchup = False , tags = [ \"example\" , \"bigquery\" ], ) as dag : run_etl = BigQueryOperator ( task_id = \"run_etl\" , sql = \"query.sql\" , destination_dataset_table = destination_table_id , write_disposition = \"WRITE_TRUNCATE\" , gcp_conn_id = \"google_cloud_conn_id\" , use_legacy_sql = False , ) validate_1 = BigQueryValueCheckOperator ( task_id = \"validate_1\" , sql = \"bigquery_value_check.sql\" , pass_value = 0 , gcp_conn_id = \"google_cloud_conn_id\" , use_legacy_sql = False , ) validate_2 = BigQueryCheckOperator ( task_id = \"validate_2\" , sql = \"bigquery_check.sql\" , gcp_conn_id = \"google_cloud_conn_id\" , use_legacy_sql = False , ) run_etl >> validate_1 >> validate_2 -- query.sql SELECT * FROM test_dataset . trips WHERE duration_minutes > 100 ; -- bigquery_value_check.sql SELECT count ( * ) FROM test_dataset . long_trips WHERE duration_minutes < 100 ; -- biquery_check.sql SELECT count ( * ) FROM test_dataset . long_trips WHERE duration_minutes > 1000 ; For more information, please refer to the GitHub Repository .","title":"Validation"},{"location":"05-validation/#validation","text":"There is a lot of ways to validate the ETL process and results. For example, dbt is a great way to validate the pipeline. However, you can utilize Airflow Operators such as BigQueryValueCheckOperator or BigQueryCheckOperator to add a simple validation task to your DAG.","title":"Validation"},{"location":"05-validation/#bigqueryvaluecheckoperator","text":"from airflow.providers.google.cloud.operators.bigquery import BigQueryValueCheckOperator check_value = BigQueryValueCheckOperator ( task_id = \"check_value\" , sql = 'validation_query.sql' , pass_value = 0 , gcp_conn_id = 'google_cloud_conn_id' , use_legacy_sql = False , ) This operator checks if the result of the SQL query is equal to pass_value . For example, if you want to validate that none of the rows in target table has duration_minutes value less than 100, you could write following query and set pass_value to 0. -- validation_query.sql SELECT count ( * ) FROM test_dataset . long_trips WHERE duration_minutes < 100","title":"BigQueryValueCheckOperator"},{"location":"05-validation/#bigquerycheckoperator","text":"from airflow.providers.google.cloud.operators.bigquery import BigQueryCheckOperator check_value = BigQueryCheckOperator ( task_id = \"check_value\" , sql = 'validation_query.sql' , gcp_conn_id = 'google_cloud_conn_id' , use_legacy_sql = False , ) This operator checks if every row of the result of the SQL query is true . For example, if you want to validate that at least one row in the target table have duration_minutes bigger than 1000, you can write the following query. -- validation_query.sql SELECT count ( * ) FROM test_dataset . long_trips WHERE duration_minutes > 1000 This validation will only fail if the count(*) ==","title":"BigQueryCheckOperator"},{"location":"05-validation/#full-code","text":"Following DAG executes ETL process with 2 validation tasks. # tutorial_dag_with_validation.py from __future__ import annotations from datetime import datetime from airflow import models from airflow.contrib.operators.bigquery_operator import BigQueryOperator from airflow.providers.google.cloud.operators.bigquery import \\ BigQueryValueCheckOperator , BigQueryCheckOperator project_id = \"starlit-sum-372013\" destination_table_id = \"test_dataset.long_trips\" with models . DAG ( dag_id = \"test_etl_dag_with_validation\" , schedule = \"@once\" , start_date = datetime ( 2022 , 12 , 1 ), catchup = False , tags = [ \"example\" , \"bigquery\" ], ) as dag : run_etl = BigQueryOperator ( task_id = \"run_etl\" , sql = \"query.sql\" , destination_dataset_table = destination_table_id , write_disposition = \"WRITE_TRUNCATE\" , gcp_conn_id = \"google_cloud_conn_id\" , use_legacy_sql = False , ) validate_1 = BigQueryValueCheckOperator ( task_id = \"validate_1\" , sql = \"bigquery_value_check.sql\" , pass_value = 0 , gcp_conn_id = \"google_cloud_conn_id\" , use_legacy_sql = False , ) validate_2 = BigQueryCheckOperator ( task_id = \"validate_2\" , sql = \"bigquery_check.sql\" , gcp_conn_id = \"google_cloud_conn_id\" , use_legacy_sql = False , ) run_etl >> validate_1 >> validate_2 -- query.sql SELECT * FROM test_dataset . trips WHERE duration_minutes > 100 ; -- bigquery_value_check.sql SELECT count ( * ) FROM test_dataset . long_trips WHERE duration_minutes < 100 ; -- biquery_check.sql SELECT count ( * ) FROM test_dataset . long_trips WHERE duration_minutes > 1000 ; For more information, please refer to the GitHub Repository .","title":"Full Code"},{"location":"06-troubleshooting/","text":"Troubleshooting This page lists common problems you might face while following the guide. docker-compose Cannot connect to the Docker daemon Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? You need a docker engine running locally to deploy Airflow via docker-compose. Check if the docker is running. Running DAGs DAG Import Errors In most cases, DAG import errors occur due to syntax errors in a DAG file. For example, it might be missing dependencies or the wrong configuration of the import path. It is best to follow the logs and debug the problem cause. Error 403: Permission denied on resource project Error: googleapi: Error 403: Permission denied on resource project {project_id}., forbidden Check if the service account has valid permissions on the target GCP project to perform the desired task.","title":"Troubleshooting"},{"location":"06-troubleshooting/#troubleshooting","text":"This page lists common problems you might face while following the guide.","title":"Troubleshooting"},{"location":"06-troubleshooting/#docker-compose","text":"","title":"docker-compose"},{"location":"06-troubleshooting/#cannot-connect-to-the-docker-daemon","text":"Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? You need a docker engine running locally to deploy Airflow via docker-compose. Check if the docker is running.","title":"Cannot connect to the Docker daemon"},{"location":"06-troubleshooting/#running-dags","text":"","title":"Running DAGs"},{"location":"06-troubleshooting/#dag-import-errors","text":"In most cases, DAG import errors occur due to syntax errors in a DAG file. For example, it might be missing dependencies or the wrong configuration of the import path. It is best to follow the logs and debug the problem cause.","title":"DAG Import Errors"},{"location":"06-troubleshooting/#error-403-permission-denied-on-resource-project","text":"Error: googleapi: Error 403: Permission denied on resource project {project_id}., forbidden Check if the service account has valid permissions on the target GCP project to perform the desired task.","title":"Error 403: Permission denied on resource project"},{"location":"07-wrap-up/","text":"Wrap Up Congratulations, you successfully integrated Airflow with BigQuery! \ud83d\udc4d This guide intentionally covers a limited variation of ETL processes/Operators to focus on the integration itself, but this will be a great starting point. If you have any suggestions on this guide, leave an issue on the GitHub repository .","title":"Wrap Up"},{"location":"07-wrap-up/#wrap-up","text":"Congratulations, you successfully integrated Airflow with BigQuery! \ud83d\udc4d This guide intentionally covers a limited variation of ETL processes/Operators to focus on the integration itself, but this will be a great starting point. If you have any suggestions on this guide, leave an issue on the GitHub repository .","title":"Wrap Up"}]}